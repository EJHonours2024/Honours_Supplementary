## Following mapping w/ Bowtie2, use Samtools to convert Fasta to Fai

samtools faidx targets.fa

## Edit and save as SAF following the example dataframe structure below. 
## Note - The length of the gene/transcript is stored on the second column of the unedited .fai file

#E.G.,
#GeneID,Chr,Start,End,Strand
#CALAbarb024696t1,CALAbarb024696t1,1,799,+
#CALAbarb044172t1,CALAbarb044172t1,1,376,+
#CALAbarb027170t1,CALAbarb027170t1,1,686,+
#CALAbarb034204t1,CALAbarb034204t1,1,469,+
#CALAbarb037942t1,CALAbarb037942t1,1,486,+
#CALAbarb036666t1,CALAbarb036666t1,1,384,+
#CALAbarb020084t1,CALAbarb020084t1,1,669,+
#CALAbarb052196t1,CALAbarb052196t1,1,1018,+
#CALAbarb052018t1,CALAbarb052018t1,1,469,+

sed -i 's/,/\t/g' targets.SAF ## Change from comma to tab-delimited after editing file. 

## Run featurecounts to quantify transcript reads using the *BestMapped.bam files produced by Bowtie2.

$HOME/tools/subread-2.0.3/bin/featureCounts -T 40 -B -C -d 100 -P -p --countReadPairs -F SAF -a targets.SAF -o read_counts *BestMapped.bam

## For DE using target accessions only, it is necessary to manually add cluster assignment before reading in 'read counts' file in R. 

#################################
########## DE Analysis ##########
#################################

R-4.3.1/bin/R

setwd("/home/darren/Emma_Honours/ancestral_de/targets_only/") ## Set working directory (example shown). 

mydata <- read.table("read_counts", sep="\t", stringsAsFactors=FALSE,  header=TRUE, row.names=1) ## Read in read counts data (edited to included cluster assignment). 

matrix <- mydata[,-c(1,2,3,4,5,6,7,11,12,13,14,15,16,21,22,23,24,25,26,30,31,32,36,37,41,42,43,44,48,49,53,54,55)] ## Select only the columns of interest for a specific contrast (here I have selected only labellum samples).

test <- aggregate(. ~ Cluster, matrix, FUN=sum) ## Aggregate single, best mapped counts to cluster level
row.names(test) <- test$Cluster ## Assign cluster column as rownames now
matrix2 <- test[,-c(1)] ## Remove cluster column and assign to new df
write.table(matrix2, file = "aggregate_sum_cluster.txt", sep="\t", row.names=FALSE)

long <- read.table("read_counts", sep="\t", stringsAsFactors=FALSE,  header=TRUE, row.names=1) ## Aggregate by longest cluster. 
new <- select(long, c(5,63)) ## Change selected columns based on size/layout of dataframe - select only the length and cluster columns. 

clus_max <- aggregate(Length~Cluster,new,function(x) x[which.max(abs(x))])

## Combine the two dataframes
test$Length <- clus_max[match(test$Cluster, clus_max$Cluster),"Length"]
write.table(test, file = "longest_cluster_length.txt", sep="\t", row.names=FALSE)

## Check here to make sure that the longest cluster length is actually correct (compare to the longest length from read counts combined with cluster table).

## Manual calculation of FPKM values using the "longest_cluster_length.txt" file. 

## FPKM = (exon mapped fragments * (10^9))/(total mapped fragments * exon length). 

## Total mapped fragments is calculated from "read_counts.summary" file produced from featurecounts - sum of "Assigned" and "Unassigned Upmapped" values for each sample. 
## Exon length information is located in the final column of hte "longest_cluster_length.txt" file. 
## Using these values, manually calculate FPKM following the above formula in an application such as LibreOffice or Excel. 

## Now, using RStudio:

## Load packages
library(readxl)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(dplyr)
library(gridExtra)
library(janitor)
library(patchwork)
library(stringr)
library(ggpubr)

## Read in manually calculated FPKM values. 

input <- read_excel("fpkm.xlsx", sheet = "example")
head(input)

## Change from wide to long data organisation. 
input_long <- gather(input, key = "File", value = "Value", -Cluster)

## Make a separate dataframe for each cluster to assess normality. 

cluster_data <- split(input_long, input_long$Cluster)

## E.G.

clus12396_data <- cluster_data[['clus12396']]
print(clus12396_data)

## Assess normality visually and statistically. 

ggqqplot(clus12396_data$Value)
shapiro.test(clus12396_data$Value) ## If p < 0.05, data does not conform to a normal distribution. 




