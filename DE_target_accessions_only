## Following mapping w/ Bowtie2, use Samtools to convert Fasta to Fai

samtools faidx targets.fa

## Edit and save as SAF following the example dataframe structure below. 
## Note - The length of the gene/transcript is stored on the second column of the unedited .fai file

#E.G.,
#GeneID,Chr,Start,End,Strand
#CALAbarb024696t1,CALAbarb024696t1,1,799,+
#CALAbarb044172t1,CALAbarb044172t1,1,376,+
#CALAbarb027170t1,CALAbarb027170t1,1,686,+
#CALAbarb034204t1,CALAbarb034204t1,1,469,+
#CALAbarb037942t1,CALAbarb037942t1,1,486,+
#CALAbarb036666t1,CALAbarb036666t1,1,384,+
#CALAbarb020084t1,CALAbarb020084t1,1,669,+
#CALAbarb052196t1,CALAbarb052196t1,1,1018,+
#CALAbarb052018t1,CALAbarb052018t1,1,469,+

sed -i 's/,/\t/g' targets.SAF ## Change from comma to tab-delimited after editing file. 

## Run featurecounts to quantify transcript reads using the *BestMapped.bam files produced by Bowtie2.

$HOME/tools/subread-2.0.3/bin/featureCounts -T 40 -B -C -d 100 -P -p --countReadPairs -F SAF -a targets.SAF -o read_counts *BestMapped.bam

## For DE using target accessions only, it is necessary to manually add cluster assignment before reading in 'read counts' file in R. 

#################################
########## DE Analysis ##########
#################################

R-4.3.1/bin/R

setwd("/home/darren/Emma_Honours/ancestral_de/targets_only/") ## Set working directory (example shown). 

mydata <- read.table("read_counts", sep="\t", stringsAsFactors=FALSE,  header=TRUE, row.names=1) ## Read in read counts data (edited to included cluster assignment). 

matrix <- mydata[,-c(1,2,3,4,5,6,7,11,12,13,14,15,16,21,22,23,24,25,26,30,31,32,36,37,41,42,43,44,48,49,53,54,55)] ## Select only the columns of interest for a specific contrast (here I have selected only labellum samples).

test <- aggregate(. ~ Cluster, matrix, FUN=sum) ## Aggregate single, best mapped counts to cluster level
row.names(test) <- test$Cluster ## Assign cluster column as rownames now
matrix2 <- test[,-c(1)] ## Remove cluster column and assign to new df
write.table(matrix2, file = "aggregate_sum_cluster.txt", sep="\t", row.names=FALSE)

long <- read.table("read_counts", sep="\t", stringsAsFactors=FALSE,  header=TRUE, row.names=1) ## Aggregate by longest cluster. 
new <- select(long, c(5,63)) ## Change selected columns based on size/layout of dataframe - select only the length and cluster columns. 

clus_max <- aggregate(Length~Cluster,new,function(x) x[which.max(abs(x))])

## Combine the two dataframes
test$Length <- clus_max[match(test$Cluster, clus_max$Cluster),"Length"]
write.table(test, file = "longest_cluster_length.txt", sep="\t", row.names=FALSE)

## Check here to make sure that the longest cluster length is actually correct (compare to the longest length from read counts combined with cluster table).
